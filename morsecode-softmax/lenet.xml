<net>
<layer idx='0' type='loss'>
<loss_multiclass_log/></layer>
<layer idx='1' type='comp'>
<fc num_outputs='5' learning_rate_mult='1' weight_decay_mult='1' bias_learning_rate_mult='1' bias_weight_decay_mult='0' use_bias='true'>
>
-0.0368001871 -0.474929005 -0.0285428148  1.08852613 -0.521396041 
0.114390157  0.31396538 -0.228209093 0.694966197 -0.737822294 
-0.609207571 0.291046143 -0.440988868 0.841335893 -0.607397139 
0.896641731  1.07837212 -0.577856421 -0.0219621956 -0.0313148797 
-0.127123997 -0.525301814 -0.69814074 0.188114047  1.56302416 
-0.218491927 -0.257945001  1.68045819 -0.578046799 -0.475805342 
-0.688662469 -0.204426438 -0.0760463551 0.202589035  0.76041168 
-0.279684722 0.00086180761 0.552590549 -0.434604436 0.160837501 
</fc>
</layer>
<layer idx='2' type='comp'>
<relu/>
</layer>
<layer idx='3' type='comp'>
<fc num_outputs='7' learning_rate_mult='1' weight_decay_mult='1' bias_learning_rate_mult='1' bias_weight_decay_mult='0' use_bias='true'>
>
-0.231314778 -0.708166659 -0.658045352 -0.370530903 0.0445544869 0.188074604 0.381330758 
-0.548987806 -0.66345197 -0.421956688 -0.261436194 -0.545457721 0.743082643 -0.262794495 
0.736540616 0.542092621 0.438188255 0.45741424 -0.571987033 -0.563534856 0.0414777212 
-0.558564246 0.277111828 0.167339444 -0.730137944 0.257155836 1.19892323 -0.510714233 
-0.145046502 -0.416167319 -0.476892352 0.489868373 -1.16037667 0.342716068 -0.431322604 
-0.353591949 -0.394350529 -0.526542962 -0.625407934 0.683054864 0.652561486 0.458621889 
0.524702847 -0.299840242 0.183830142 -0.646151602 0.48128438 -0.632027924 0.21740672 
-0.76506108 0.212867185 0.158796743 0.248300225 -0.707580149 0.508483946 -0.548723102 
-0.159264296 0.134856001 0.0217146613 0.359823555 0.461997718 0.536159158 0.456135064 
</fc>
</layer>
<layer idx='4' type='comp'>
<htan/>
</layer>
<layer idx='5' type='comp'>
<fc num_outputs='8' learning_rate_mult='1' weight_decay_mult='1' bias_learning_rate_mult='1' bias_weight_decay_mult='0' use_bias='true'>
>
0.668793023 0.378660262 0.0946520045 -0.497348607 -0.265682042 0.536383033 0.340878725 0.230475292 
-0.25393787 -0.344868779 -0.283098787 0.454825372 -0.434500396 0.0958499014 0.453898042 -0.587036371 
0.0196266733 -0.0734767169 0.276181519 0.0887105614 -0.0886614993 -0.210896462 -0.130512446 -0.0484209955 
-0.0396273769 -0.092005454 0.281221598 -0.133159071 -0.506719947 0.418367207 0.338366747 -0.353425205 
0.149968982 -0.728728771 0.123232745 -0.341395974 0.0113559077 0.177081749 0.547341764 -0.281260341 
0.465924233 0.103085436 -0.572441995 0.565988481 -0.0637324303 0.713532329 0.120463021 -0.127156496 
0.0627683476 0.0173426773 -0.172581643 -0.0608117431 0.367319971 -0.249166653 -0.160044745 0.200427756 
-0.0383354016 -0.0806610435 -0.429706752 0.330535233  0.60021764 0.411790699 -0.366833568 0.546167016 
-0.531283855 -0.287760049 0.657961965 -0.237671033 0.170982257 -0.100709014 0.0421265438 -0.239771768 
0.129917651  0.48434481 -0.63168931 0.534347713 0.296512812 0.572540164 -0.00849083904  0.42793107 
0.0440962911 0.0584036335 0.0946835279 -0.205530927 -0.0555881336 0.436112314 -0.0358468443 -0.112329304 
</fc>
</layer>
<layer idx='6' type='comp'>
<htan/>
</layer>
<layer idx='7' type='comp'>
<fc num_outputs='10' learning_rate_mult='1' weight_decay_mult='1' bias_learning_rate_mult='1' bias_weight_decay_mult='0' use_bias='true'>
>
-0.161299706  0.212680757 0.0704781562  0.118870363  0.263319105   1.52621055  0.164664581 -0.372586489  -1.01614738   1.07773435 
-0.359276354 -0.525835812 -0.019541299 0.0554092675  0.157895148  0.620630562  0.350116879 -0.0426978059 -0.0153381974 0.0183247775 
-0.00979667902 0.00335759018  0.229186326  0.350686848  0.198809594 0.0199213661 -0.369963884 -0.370220363   0.27652806 -0.0994481742 
 0.431822419  0.169247761  0.585272133 -0.111036867  0.371681184  0.545324743  0.172618926  0.374063462  0.195391625 -0.0152841723 
-0.0651874319 0.0208767615 -0.167220742 -0.0759213939  0.401448876 -0.0385843031  0.048711542 -0.265885353  0.259433299 -0.0259710122 
-0.153904378 0.0716213733 0.0844891369 0.0445997007 -0.494206697  0.668986619   0.17407082 -0.229407504  0.134633273 -0.00997214578 
 -0.41241011 -0.0530477725  0.476652235 -0.0661973059  0.117936008 -0.0568815209  0.251167923  -0.28103134  0.223961338 -0.0835473239 
-0.336466402 -0.255202174 -0.0133271171  0.297408938  0.313047886  0.529115677  0.425236046  0.198855489 0.0777987316  0.568400085 
-0.211912185 0.0138487481  0.165583968 0.0723273307 0.00636090152 -0.0654399022  0.277524203  0.336599201  0.194067284 -0.045386035 
-0.459546804 -0.539509416 -0.116519153 -0.410814285 -0.451247424 0.0658989772  0.209898829  0.224571928 -0.121930458  0.855852067 
-0.0235130321 -0.00244680326 -0.0347287543 0.00196801848 -0.000639096601 -0.986056447 -0.00258062594 0.00481286738  0.334529161 -0.745706499 
</fc>
</layer>
<layer idx='8' type='input'>
<input/></layer>
</net>
